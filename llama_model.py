# -*- coding: utf-8 -*-
"""llama_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jxSZEk1wqW45YDTeiGXVxAAa3IPttLXb
"""

from ctransformers import AutoModelForCausalLM

# âœ… Correct: model path is the first argument
llm = AutoModelForCausalLM.from_pretrained(
    "model/llama-2-7b-chat.ggmlv3.q8_0.bin",  # This is the required positional argument
    model_type="llama",
    temperature=0.7
)

def getLlamaResponse(topic, no_of_words, blog_style):
    prompt = f"Write a {blog_style} blog post about {topic} in about {no_of_words} words."
    response = llm(prompt)
    return response